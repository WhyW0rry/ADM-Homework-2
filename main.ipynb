{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Group #3\n- Beatrice Nobile\n- Bertrand Leclercq\n- Théophile Tolani"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom io import StringIO\nfrom csv import writer\nimport datetime","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import data\nImport the data and create a Pandas data frame with it."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"directory = \"../input/ecommerce-behavior-data-from-multi-category-store/2019-Oct.csv\"\ndirectory2 = \"../input/ecommerce-behavior-data-from-multi-category-store/2019-Nov.csv\"\n\noctober = pd.read_csv(directory, header='infer', parse_dates=['event_time'], date_parser=pd.to_datetime, nrows=1)\nnovember = pd.read_csv(directory2, header='infer', parse_dates=['event_time'], date_parser=pd.to_datetime)\ndata = pd.concat([october, november], ignore_index=True)\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Question 1\n\n***A marketing funnel describes your customer’s journey with your e-commerce. It may involve different stages, beginning when someone learns about your business, when he/she visits your website for the first time, to the purchasing stage, marketing funnels map routes to conversion and beyond. Suppose your funnel involves just three simple steps: 1) view, 2) cart, 3) purchase. Which is the rate of complete funnels?***\n\nSince we don't need all the columns, we are only going to consider the ones we care about, namely [event_type, product_id, user_id]. Once we have these, we're probably going to have a lot of duplicates, since each user could have viewed, added to cart or purchased a product more than once, but what we are interested in is only if they have undergone the complete process, so we can drop all of these duplicates. Once we have the cleaned dataset, we can see whether each combination of [product_id, user_id] has all three event types or not and compute the rate of complete funnels."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select the data we need and clean it \nsub_df = data.loc[:, [\"event_type\", \"product_id\", \"user_id\"]]\ntot_events = sub_df.drop_duplicates().shape[0]\nsub_df = sub_df.drop_duplicates().groupby(['product_id', 'user_id'])\n\ncomplete_funnels = 0\n\n# Check if all three events have occurred for each [product_id, user_id] combination\nfor idx, group1 in sub_df:\n    if len(group1) == 3:\n        complete_funnels += 1\n\nrate_complete_funnels = round(complete_funnels/tot_events, 3)\n\nprint(\"The rate of complete funnels is:\", rate_complete_funnels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we go on, since the dataset is so big and we will be creating sub-datasets in order to avoid working with the whole of it, and also to filter our data according to our needs, we are going to delete anything we create as soon as we use it, so as to free up memory usage."},{"metadata":{"trusted":true},"cell_type":"code","source":"lst = [october, november, sub_df]\ndel october, november, sub_df\ndel lst","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Question 1.1\n\n***What’s the operation users repeat more on average within a session? Produce a plot that shows the average number of times users perform each operation (view/removefromchart etc etc).***"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We count each event type per each session\nevent_per_session = pd.DataFrame(data.value_counts(subset = [\"user_session\", \"event_type\"], \n                                                   sort = False), columns = [\"count\"])\nevent_per_session.reset_index(level=[\"event_type\"], inplace=True)\n\n# We then groupby event type and compute the mean\naverages = event_per_session.groupby(by = \"event_type\", sort = False).mean().to_dict()\naverages = averages[\"count\"]\n\n# Plot\nplt.bar(*zip(*averages.items()), width= 0.6, color = [\"darkblue\", \"blue\", \"lightblue\"])\nplt.xlabel(\"Event Types\", fontdict={'fontsize': 14}) \nplt.ylabel(\"Averages\", fontdict={'fontsize': 14}) \nplt.title(\"Events' Occurrences per Session\", fontdict={'fontsize': 16})\nplt.savefig(\"Events' Occurrences per Session\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del event_per_session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Question 1.2\n\n***How many times, on average, a user views a product before adding it to the cart?***\n\nFirst of all, we need to take into consideration the fact that a user might view the product even after adding it to cart or even after purchasing it. Below we see an example:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[(data[\"product_id\"] == 5100816) & (data[\"user_id\"] == 550121407)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we need to understand how many times the user views the product **before** adding it to the cart, we must count only those occurrences that come before that event. We first select only the columns we are interested in, and create two groups, one for the \"view\" event, and one for the \"cart\" event. Then we compare the two groups, merge them on the base of the product_id and the user_id and then subtract from the resulting dataframe the rows that come after the time when the product was added to the cart. Finally, we count those rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"# We consider only those columns we need and for future questions, we convert event_time\n# from string to datetime.\nsub_df = data.loc[:, [\"event_time\", \"event_type\", \"product_id\", \"user_id\"]]\n\n# We create the groups we need \ncart_products = sub_df.query('event_type==\"cart\"').product_id\n\ngroup_cart = sub_df.query('event_type==\"cart\"').groupby(['user_id', 'product_id'])\n\ngroup_view = (sub_df[sub_df.product_id.isin(cart_products)]\n              .query('event_type==\"view\"').groupby(['user_id', 'product_id']))\n\n# We retrieve the views for each [product_id, user_id] combination that has added \n# to cart the product. Once we have them, we consider only those views that occurred\n# before adding to cart.\ncounts = []\nfor idx, group1 in group_cart:\n    try:\n        group2 = group_view.get_group(idx)\n    except KeyError:\n        # handle rare case of a cart event with no view event\n        counts.append(0)\n        continue\n    counts.append(group2.loc[group2.event_time.lt(group1.event_time.iloc[0]), 'event_type'].count() )\n\nresult_1_2 = round(sum(counts)/len(counts), 2)\nprint(f\"On average an user views the product {result_1_2} times before adding it to the cart.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lst = [cart_products, group_cart, group_view]\ndel cart_products, group_cart, group_view\ndel lst","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Question 1.3\n\n***What’s the probability that products added once to the cart are effectively bought?***\n\nWe already have the sub_dataframe with only the columns we are interested on (sub_df). We also already know which are the products that have been added to the cart by users (cart_products). Based on this group of products, we can see which are the products that have been purchased after being carted. Then, we use exactly the same method as before, but now, we check for what has appened after adding the product to the cart. Another difference with respect to before is that we are not really interested in knowing how many times they purchase the product, but only if they do.\n\nIn the mean time, we prepare the ground for question 1.4, and we extract all the information on those customers that have added to the cart a certain product, but have not proceeded into purchasing it."},{"metadata":{"trusted":true},"cell_type":"code","source":"group_purchased = (sub_df[sub_df.product_id.isin(cart_products)]\n              .query('event_type==\"purchase\"').groupby(['user_id', 'product_id']))\n\nbought = []\n\n# The following method is the fastest way we have found to create the subgroups we need.\n# In this case, all those products that have been added to cart, and subsequently purchased\n# or removed from cart.\noutput = StringIO()\ncsv_writer = writer(output)\n\nfor idx, group1 in group_cart:\n    try:\n        group2 = group_purchased.get_group(idx)\n    except KeyError:\n        # these are the cases of 'cart' events without 'purchase' events \n        bought.append(0)\n        csv_writer.writerow(group1.to_numpy()[0])\n    if group2.loc[group2.event_time.gt(group1.event_time.iloc[0]), 'event_type'].count() > 0:\n        bought.append(1)\n        \noutput.seek(0) \nremoved = pd.read_csv(output, header=None, sep='\\n')\nremoved = removed[0].str.split(',', expand=True)\nremoved.columns = [\"event_time\", \"event_type\", \"product_id\", \"user_id\"]\nremoved[[\"product_id\", \"user_id\"]] = removed[[\"product_id\", \"user_id\"]].apply(pd.to_numeric)\n\nresult_1_3 = round(sum(bought)/len(bought), 2)\n\nprint(\"The probability of purchasing a product after having added it to the cart is:\", result_1_3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del group_purchased","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Question 1.4\n\n***What’s the average time an item stays in the cart before being removed?***\n\nThe first thing we notice here is that the dataset doesn't actually contain \"removefromcart\" event. Yet, as suggested by the author of the dataset, we can figure out removed from cart products as the difference between purchase and cart events. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[data[\"event_type\"] == \"removefromcart\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, we can't use aggregate functions, because we need to consider the timing. A product can be classified as \"removed from cart\" only after it has been added to cart and afterwards it has not been purchased, and that is what we have built in the \"removed\" dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"removed","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Quick check to see if indeed the combinations [\"product_id\", \"user_id\"] in the removed dataframe are indeed those that have added the products on the cart but not purchased them afterwards."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[(data[\"product_id\"] == 4700419) & (data[\"user_id\"] == 107837897)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So now we know which products that had been added to the cart have not afterwards been purchased, and hence that at some point must have been removed from the cart. To compute how much time the product has stayed in the cart, we can look at the last time the user has viewed the product, after having added it to the cart, and once he/she has not viewed it anymore, then we suppose that they have removed it from the cart. For those that do not view the product after the cart, we assume that the element has immediately been removed from the cart.\n\nTherefore, we now need to retreive the last time the product has been viewed for each [user_id, product_id] pair that has been classified as \"removed_from_cart\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"# We collect the data we need with the conditions we need, and in particular \n# only the last views for each [product_id, user_id] combination.\n\nremoved_products = list(removed.product_id)\n\ngroup_removed = removed.groupby(['product_id', 'user_id'])\n\ngroup_view = (sub_df[sub_df.product_id.isin(removed_products)]\n              .query('event_type==\"view\"').groupby(['user_id', 'product_id']).last())\ngroup_view = group_view.groupby(['product_id', 'user_id'])\n\n# We use the same method to get the subgroup of those products that have been removed\n# from cart and then viewed.\noutput = StringIO()\ncsv_writer = writer(output)\n\nfor idx, group1 in group_removed:\n    try:\n        group2 = group_view.get_group(idx)\n        csv_writer.writerow(group2.to_numpy()[0])\n    except KeyError:\n        # cases where the combination [product_id, user_id] has not been removed\n        continue\n    \noutput.seek(0) \nlast_viewed = pd.read_csv(output, header=None, sep='\\n')\nlast_viewed = last_viewed[0].str.split(',', expand=True)\nlast_viewed.columns = ['user_id', \"product_id\", \"event_time\", \"event_type\"]\nlast_viewed[[\"product_id\", \"user_id\"]] = last_viewed[[\"product_id\", \"user_id\"]].apply(pd.to_numeric)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"last_viewed.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once we know also the last view for each [product_id, user_id] combination where the product has been removed from the cart, we can then concatenate the two groups and take the time difference for each occurrence and then compute the average."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now the filtered data can be combined and grouped.\ncart_last_view = pd.concat([removed, last_viewed])\ng_cart_last_view = cart_last_view.groupby([\"product_id\", \"user_id\"])\n\ntime_in_cart = []\n\n# For each [product_id, user_id] combination, we look for the time of the last view\n# and when the user had added to the cart the product. If the user's last view is\n# before the cart event, we assume the product has been immediately removed. \nfor idx, group1 in group_removed:\n    try:\n        group2 = g_cart_last_view.get_group(idx)\n        if list(group2.event_time)[1] > list(group2.event_time)[0]:\n            time_in_cart.append(((pd.Timedelta((list(group2.event_time)[1] #Timedelta.value is in nanoseconds\n                                               - list(group2.event_time)[0])).value)/1000000000))\n        else:\n            time_in_cart.append(0)\n    except IndexError:\n        # cases where the product has not been viewed \n        time_in_cart.append(0)\n        \n        \nresult_1_4 = sum(time_in_cart) / len(time_in_cart)\n\nsec = datetime.timedelta(seconds = result_1_4)\nd = datetime.datetime(1,1,1) + sec\n\nprint(f\"On average before being removed a product stays in the cart:\")\nprint(\"%d Day and %d:%d:%d\" % (d.day-1, d.hour, d.minute, d.second))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lst = [removed, group_removed, group_view, last_viewed, cart_last_view, g_cart_last_view]\ndel removed, group_removed, group_view, last_viewed, cart_last_view, g_cart_last_view\ndel lst","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Question 1.5\n\n***How much time passes on average between the first view time and a purchase/addition to cart?***\n\nTo answer this question, we first create three different dataframes that are grouped by the [product_id, user_id] combination, and that contain only the first occurrence of each event type. Indeed, a user may buy a product more than once, may also add it to cart, then remove it, and then add it again, etc...\n\nOnce we have these subgroups, we can compute the time difference between each event's first occurrence."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter the date according to needs: get the first occurrence for each event.\nfirst_view = (sub_df.query('event_type==\"view\"').groupby(['user_id', 'product_id']).first()).groupby(['user_id', 'product_id'])\nfirst_cart = (sub_df.query('event_type==\"cart\"').groupby(['user_id', 'product_id']).first()).groupby(['user_id', 'product_id'])\nfirst_purchase = (sub_df.query('event_type==\"purchase\"').groupby(['user_id', 'product_id']).first()).groupby(['user_id', 'product_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute time difference between first view and cart event.\ntime_cart = []\n\nfor idx, group1 in first_cart:\n    try:\n        group2 = first_view.get_group(idx)\n        if list(group1.event_time)[0] > list(group2.event_time)[0]:\n            time_cart.append((pd.Timedelta((list(group1.event_time)[0] \n                                            - list(group2.event_time)[0])).value)/1000000000) \n    except:\n        continue\n\n        \nresult_1_5_1 = sum(time_cart) / len(time_cart)\n\nsec_1 = datetime.timedelta(seconds = result_1_5_1)\nd_1 = datetime.datetime(1,1,1) + sec_1\n\nprint(f\"On average, the time it takes a user to add a product to the cart after the first view is:\")\nprint(\"%d Days and %d:%d:%d\" % (d_1.day-1, d_1.hour, d_1.minute, d_1.second))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute time difference between first view and purchase event.\ntime_purchase = []\n\nfor idx, group1 in first_purchase:\n    try:\n        group2 = first_view.get_group(idx)\n        if list(group1.event_time)[0] > list(group2.event_time)[0]:\n            time_purchase.append((pd.Timedelta((list(group1.event_time)[0] \n                                            - list(group2.event_time)[0])).value)/1000000000)\n    except:\n        continue\n        \n\nresult_1_5_2 = sum(time_purchase) / len(time_purchase)\n\nsec_2 = datetime.timedelta(seconds = result_1_5_2)\nd_2 = datetime.datetime(1,1,1) + sec_2\n\nprint(f\"On average, the time it takes a user to purchase a product after the first view is:\")\nprint(\"%d Days and %d:%d:%d\" % (d_2.day-1, d_2.hour, d_2.minute, d_2.second))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lst = [first_view, first_cart, first_purchase, sub_df]\ndel first_view, first_cart, first_purchase, sub_df\ndel lst","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Question 2\n\n***What are the categories of the most trending products overall? For each month visualize this information through a plot showing the number of sold products per category.***\n\nFor this question, we need only the data from the sold products, which mean the data where the event_type is 'purchase'. For the category, we use the category_code which is the meaningful name of the category (in the format category.subcategory, so we apply a function to extract only the category name). The best visualisation for this data is the \"bar\" plot."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Use only the data from purchase (= sold products) \npurchases = data[data.event_type == 'purchase'].copy()\n\n# Extract the category name from the category_code attribute\npurchases['category_code'] = purchases['category_code'].apply(lambda x: str(x).split('.')[0])\n\npurchasesByMonth = purchases.groupby([purchases.event_time.dt.month])\n\n# Group the data by month and for each month, make a plot\nfor month, frame in purchasesByMonth:\n    \n    categoriesCount = frame['category_code'].value_counts()\n    \n    x = list(map(str, categoriesCount.index))\n    heigths = categoriesCount.values\n    \n    plt.figure(figsize=(21,7))\n    plt.bar(x, heigths)\n    plt.title(f'Month #{month}')\n    plt.xticks(rotation=90)\n    plt.show()\n    \ndel purchases","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Question 2.1\n\n***Plot the most visited subcategories.***\n\nThis is similar from the previous question, except that we need the visits and not the sold data (so the data where the event_type is 'view') and we want all subcategories (and not only categories)."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Use only the data from view (= visited products) \nvisits = data[data.event_type == 'view'].copy()\n\n# Group the data by month and for each month, make a plot\nfor month, frame in visits.groupby([visits.event_time.dt.month]):\n    \n    categoriesCount = frame['category_code'].value_counts()\n    \n    x = list(map(str, categoriesCount.index))\n    heigths = categoriesCount.values\n    \n    plt.figure(figsize=(21,7))\n    plt.bar(x, heigths)\n    plt.title(f'Month #{month}')\n    plt.xticks(rotation=90)\n    plt.show()\n    \n# Delete variables that will not be used in next questions\ndel visits","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Question 2.2\n\n***What are the 10 most sold products per category?***\n\nFor this question, I reuse the `purchasesByMonth` variable from the question 2. For each month, we group the data by category and then for each category, we counts the number of product and print the 10 higher number whith the id of the product."},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"for month, frame in purchasesByMonth:\n    \n    print(f'Month {month}')\n    print('----------------------------------')\n    \n    for categoryCode, frame2 in frame.groupby([frame.category_code]):\n        \n        print(f'Categorie: {categoryCode}')\n        print(frame2['product_id'].value_counts().head(10))\n        print()\n\n# Delete variables that will not be used in next questions\ndel purchasesByMonth","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"\n# Question 3\n\n***For each category, what’s the brand whose prices are higher on average?***\n\n# Question 3.1\n\n***Write a function that asks the user a category in input and returns a plot indicating the average price of the products sold by the brand.***\n\nFor this question, we fist extract the category name from the category_code attribute, print all possible category and wait for an input from the user (which need to be in the list of category). Then, we extract the data from the given category, group the category data by brand and compute the price mean before plotting the result in a 'bar' plot (which is the more suitable for this visualisation)."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataCopy = data.copy()\n\n# Extract the category name from the category_code attribute\ndataCopy['category_code'] = dataCopy['category_code'].apply(lambda x: str(x).split('.')[0])    \n\n# Ask the user for a category and check if it exists\nwhile True:\n    print(f'Enter a category ({\", \".join(dataCopy[\"category_code\"].unique())}):')\n    category = input()\n    if category in dataCopy[\"category_code\"].unique():\n        break\n\n# Extract the data from the given category\ncategoryData = dataCopy[dataCopy.category_code == category]\n# Group the category data by brand and compute the price mean\nbrandPriceAverage = categoryData.groupby([categoryData.brand]).price.mean()\n\n# Plot the price mean / brand\nx = brandPriceAverage.index\nheigths = brandPriceAverage.values\n\nplt.figure(figsize=(21,7))\nplt.bar(x, heigths)\nplt.title(f'Average price by brand for \"{category}\" products')\nplt.xticks(rotation=90)\nplt.show()\n\n# Delete variables that will not be used in next questions\ndel category, categoryData, brandPriceAverage, x, heigths","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Question 3.2\n\n***Find, for each category, the brand with the highest average price. Return all the results in ascending order by price.***\n\nFor this question, we group the data by category and by brand and we compute the price mean. The for each category, we apply a sort_value to sort the brand and their mean price in descending order and get the first one (thus the brand with the highest mean price). After that, we sort the category in ascending order."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataCopy = dataCopy.groupby([dataCopy.category_code, dataCopy.brand]).price.mean()\ndataCopy.groupby('category_code', group_keys=False).apply(lambda x: x.sort_values(ascending=False).head(1)).sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Delete variables that will not be used in next questions\ndel dataCopy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Question 4\n\n***How much does each brand earn per month? Write a function that given the name of a brand in input returns, for each month, its profit. Is the average price of products of different brands significantly different?***"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Again, we select only those columns we need, and since we want to know about profits\n# we will only consider the purchase event. Moreover, since we don't care about the time\n# of purchase, but only about its month, we have changed the format of the dates.\nsub_df = data.loc[:, [\"event_time\", \"event_type\", \n                      \"brand\", \"price\"]].query(\"event_type == 'purchase'\")\nsub_df['event_time'] = pd.to_datetime(sub_df[\"event_time\"].str.slice(0, -13), format = '%Y-%m-%d')\nsub_df = sub_df.set_index(pd.DatetimeIndex(sub_df['event_time']))\nsub_df.drop(['event_time', 'event_type'], axis = 1, inplace = True)\n\n# We group by months with grouper, and then group by brands to take the mean price\nsub_df = pd.DataFrame(sub_df.groupby([pd.Grouper(freq='M'), 'brand'])['price'].mean())\nsub_df.reset_index(level='brand', inplace = True)\n\n# We take notice of those brands that are in the dataset, but do not seem to make a profit\n# in the two months considered.\nbrands = list(sub_df.brand.drop_duplicates())\nall_brands = list(data.brand.drop_duplicates())\nif len(brands) != len(all_brands):\n    missing_brands = [i for i in all_brands if i not in brands]\n\n# Since we will need to know if the month is October or November, we thought it easier\n# and more understandable to change it again into a string and replace the value with \n# the months' names\nsub_df.reset_index(inplace = True)\nsub_df['event_time'] = sub_df['event_time'].dt.strftime('%Y-%m-%d')\nsub_df['event_time'] = sub_df.replace({'event_time': {'2019-10-31': 'october', '2019-11-30': 'november'}})\ng_sub_df = sub_df.groupby('brand')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def brand_monthly_profit(brand):\n    group = g_sub_df.get_group(brand)\n    if brand in brands:\n        profits = {}\n        for i in range(len(group)):\n            profits[list(group.event_time)[i]] = list(group.price)[i]\n        return profits\n    else:\n        print(f\"{brand}'s profits for October and November are 0\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"brand_monthly_profit('aqua')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = round(sub_df.price.mean(), 2)\nmini = round(sub_df.price.min(), 2)\nmaxi = round(sub_df.price.max(), 2)\nvar = round(sub_df.price.var(), 2)\n\nprint(\"The average profits change greatly among brands, as the variance shows.\")\nprint(f\"In fact, the variance is {var}, and indeed while the mean is {mean}, we have maximum value {maxi} and minimum {mini}.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Question 4.1\n\n***Using the function you just created, find the top 3 brands that have suffered the biggest losses in earnings between one month and the next, specifing both the loss percentage and the 2 months (e.g., brand_1 lost 20% between march and april).***"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We consider only those brands that actually made a profit in both months\nlosses = {}\nworst_brands = [0, 0, 0]\nbrand_names = ['', '', '']\n\nfor i in brands:\n    profits = brand_monthly_profit(i)\n    if len(profits) == 2: # checking if they made a profit in both months\n        if profits['october'] > profits['november']: #checking if they made a loss \n            \n            # We immediately compare the results and save only those \n            #  3 that feature the biggest losses.\n            if (profits['october'] - profits['november']) > min(worst_brands):\n                idx = worst_brands.index(min(worst_brands))\n                worst_brands[idx] = profits['october'] - profits['november']\n                brand_names[idx] = i\n                losses[i] = round((profits['november'] \n                                   - profits['october'])/profits['october']*100, 2)    \n    else:\n        continue\n\nfor bran in brand_names:\n    print(f\"{bran} lost {losses[bran]}% between October and November.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Question 5\n\n***In what part of the day is your store most visited? Knowing which days of the week or even which hours of the day shoppers are likely to visit your online store and make a purchase may help you improve your strategies. Create a plot that for each day of the week show the hourly average of visitors your store has.***\n\n##### Solution 1\nSolved using .groupby() to split our data. In order to know how many people are accessing the website I decided to rely on the number of user sessions accessing the website. We first split by day in order to count our data day per day. Then we count how many user sessions are accessing the website and group them for each hour. After that we only have to plot the results for each day on one single graph. We can see that there are clearly two attendance peaks : one around 8am in the morning and one around 5pm in the afternoon."},{"metadata":{"trusted":true},"cell_type":"code","source":"# First Group by Day\nfor group in data.groupby([data.event_time.dt.day]):\n    \n    #Then Group by hours for each day and count each user_session\n    dfp = group[1].groupby([data.event_time.dt.hour]).user_session.count()\n    \n    #Plot\n    dfp.plot()\n    plt.title(label = 'Number of visits per hour')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Solution 2\n\nSolved using a .resample() method instead of .groupby(). Works with the same logic as the previous solution and comes to the same conclusions."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = data[['event_time','product_id','event_type']]\n\ndf = df.set_index(['event_time','product_id']).count(level=\"event_time\")\n\ndf.resample('H').mean().plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Question 6\n\n***The conversion rate of a product is given by the number of times a product has been bought over the number of times it has been visited. What's the conversion rate of your online store?***\n\n# Question 6.1\n\n***Find the overall conversion rate of your store.***\n\n# Question 6.2\n\n***Plot the number of purchases of each category and show the conversion rate of each category in decreasing order.***"},{"metadata":{"trusted":true},"cell_type":"code","source":"a = len(data[data['event_type']=='view'])\nprint('The website has been visited '+str(a)+' times.')\nb = len(data[data['event_type']=='purchase'])\nprint('There were '+str(b)+' purchases.')\nprint('The overall conversion rate of the store is then :', str(b/a))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"purch = data[data.event_type == 'purchase'].groupby('category_id').count()\npurch = purch[['event_type']]\nplt.scatter(np.linspace(0,len(purch),len(purch)),purch)\nplt.title('Number of purchases for each category')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"purch.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"views = data[data.event_type == 'view'].groupby('category_id').count()\nviews = views[['event_type']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"L = []\n\n# By using the length of the dataframe 'purch' (which contains all the purchases for each category), \n# we drop every category with no purchase.These have a conversion rate of 0. \nfor k in range(len(purch)):\n    L.append(int(purch.iloc[k])/int(views.iloc[k]))\n\nL.sort(reverse = True)\n\npurch['Conversion Rate'] = L","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Conversion Rate of each category in decreasing order')\npurch.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Question 7\n\n***The Pareto principle states that for many outcomes roughly 80% of consequences come from 20% of the causes. Also known as 80/20 rule, in e-commerce simply means that most of your business, around 80%, likely comes from about 20% of your customers. Prove that the pareto principle applies to your store.***"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = data.loc[:, [\"event_type\", \"brand\", \"price\"]].query(\"event_type == 'purchase'\")\nall_customers = set(sub_df.brand.dropna())\nsales = pd.DataFrame(sub_df.groupby('brand')['price'].sum()).sort_values(by= \"price\", \n                                                                         ascending = False)\nsales.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"percentage_customer = int(0.2*len(all_customers)) #543\ntot_sales = int(sales.sum()) #485584186\ntarget_sales = int(0.8*tot_sales) #388467348\nsales_20perc = int(sales[0:percentage_customer].sum()) #482125166\nsales_generated_20perc = round(sales_20perc/tot_sales*100, 2) #99.29%","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if sales_20perc > target_sales:\n    print(f\"20% of the online store customers create {sales_generated_20perc}% of the store's total sales.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = data.loc[:, [\"event_type\", \"user_id\", \"price\"]].query(\"event_type == 'purchase'\")\nall_customers = set(sub_df.user_id.dropna())\nsales = pd.DataFrame(sub_df.groupby('user_id')['price'].sum()).sort_values(by= \"price\", \n                                                                         ascending = False)\nsales.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"percentage_customer = int(0.2*len(all_customers)) #139494\ntot_sales = int(sales.sum()) #505152392\ntarget_sales = int(0.8*tot_sales) #404121913\nsales_20perc = int(sales[0:percentage_customer].sum()) #364222233\nsales_generated_20perc = round(sales_20perc/tot_sales*100, 2) #72.1%","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(percentage_customer, tot_sales, target_sales, sales_20perc, sales_generated_20perc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if sales_20perc > target_sales:\n    print(f\"20% of the online store customers create {sales_generated_20perc}% of the store's total sales.\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}